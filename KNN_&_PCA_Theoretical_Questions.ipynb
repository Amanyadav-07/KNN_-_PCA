{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. What is K-Nearest Neighbors (KNN) and how does it work?\n"
      ],
      "metadata": {
        "id": "eaYQatNfBxLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for classification and regression problems. It is based on the principle that similar things exist in close proximity.\n",
        "\n",
        "####How it Works:-\n",
        "\n",
        "```\n",
        "1. Choose the value of K\n",
        "2. Calculate the distance between the new data point and all other points in the dataset using distance measures like Euclidean distance, Manhattan distance, or Minkowski distance\n",
        "3. Find the K nearest neighbors (smallest distances).\n",
        "4. For classification: Assign the most common class among the K neighbors.\n",
        "5. Output the predicted value or class.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "F6BLWej3B-t8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. What is the difference between KNN Classification and KNN Regression?\n"
      ],
      "metadata": {
        "id": "gUN-r5MnCbDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Difference B/w KNN Classification and KNN Regression:-\n",
        "\n",
        "####KNN Classification:-\n",
        "\n",
        "```\n",
        "1. Predicts a category/class (e.g., \"Apple\" or \"Orange\").\n",
        "2. Assigns the most common class among K neighbors (majority voting).\n",
        "3. Classifying an email as Spam or Not Spam.\n",
        "4. Used in categorical data problems.\n",
        "5. Uses mode (most frequent class).\n",
        "```\n",
        "\n",
        "####KNN Regression:-\n",
        "\n",
        "```\n",
        "1. Predicts a continuous value (e.g., house price, temperature).\n",
        "2. Takes the average (or weighted average) of K nearest values.\n",
        "3. Predicting the price of a house based on nearby houses.\n",
        "4. Used in numerical data problems.\n",
        "5. Uses mean (average value).\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cDTNpJfhCkl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. What is the role of the distance metric in KNN?"
      ],
      "metadata": {
        "id": "GYpMKHClDL_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- The distance metric in KNN plays a crucial role in determining how similar (or different) data points are. Since KNN is based on finding the K-nearest neighbors, the way we measure distance directly affects the accuracy of the algorithm.\n",
        "\n",
        "####The smaller the distance, the more similar the data points are, and the larger the distance, the less similar they are.\n",
        "\n",
        "####Types of Distance Metrics in KNN:-\n",
        "\n",
        "```\n",
        "1. Euclidean Distance (Most Common)\n",
        "2. Manhattan Distance (City Block Distance)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ukH5LZiFDXbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. What is the Curse of Dimensionality in KNN?\n"
      ],
      "metadata": {
        "id": "Dop_yjxeDrug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- The Curse of Dimensionality refers to the problem that arises when the number of features (dimensions) increases, making distance-based algorithms like KNN less effective.\n",
        "\n",
        "####In high-dimensional spaces, data points become sparse (spread far apart), making it difficult for KNN to find meaningful nearest neighbors.\n",
        "\n"
      ],
      "metadata": {
        "id": "BMnewg8_D2pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. How can we choose the best value of K in KNN?"
      ],
      "metadata": {
        "id": "FIWA-RjyD5cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- The value of K (number of neighbors) is a crucial hyperparameter in KNN. Choosing the right K affects the accuracy and performance of the model.\n",
        "\n",
        "\n",
        "####Methods to Choose the Best K:-\n",
        "```\n",
        "1. Try Different K values and Use Cross-Validation.\n",
        "2. Use the Elbow Method.\n",
        "3. Consider the Dataset Size\n",
        "4. Avoid Overfitting & Underfitting\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "I2hnbec4EFM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. What are KD Tree and Ball Tree in KNN?"
      ],
      "metadata": {
        "id": "-WFeJoa-Eicm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- KD Tree(K-Dimensional Tree):- A KD Tree is a space-partitioning data structure used to organize points in a K-dimensional space. It recursively divides the space into two half-spaces using hyperplanes, making nearest neighbor searches in low-dimensional data more efficient.\n",
        "\n",
        "####Ball Tree:- A Ball Tree is a data structure used for organizing points in a multi-dimensional space by grouping them into nested hyperspheres (balls). It is particularly useful for high-dimensional data, as it allows efficient nearest neighbor searches by eliminating large portions of the search space."
      ],
      "metadata": {
        "id": "2SJIWDmQE5J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. When should you use KD Tree vs Ball Tree?"
      ],
      "metadata": {
        "id": "HlDWlDZUFF-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- When to use KD Tree and Ball Tree:-\n",
        "\n",
        "####KD Tree:-\n",
        "```\n",
        "1. Best for low to moderate dimensions(â‰¤ 20 features).\n",
        "2. Works well when data is evenly distributed.\n",
        "3. Faster for Euclidean distance-based searches.\n",
        "4. Suitable for geographic data, 2D/3D spatial searches(e.g., nearest resturent search).\n",
        "```\n",
        "\n",
        "####Ball Tree:-\n",
        "```\n",
        "1. Best for high-dimensional data (> 20 features).\n",
        "2. Works well when data is unevenly distributed.\n",
        "3. Supports various distance metrics (Euclidean, Manhattan, Minkowski, etc.).\n",
        "4. Used in image processing, text classification, machine learning feature spaces.\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ncG8_CNJFP4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. What are the disadvantages of KNN?\n"
      ],
      "metadata": {
        "id": "KDHLpkWAGKPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Disadvantages of K-Nearest Neighbors(KNN):-\n",
        "\n",
        "\n",
        "```\n",
        "1. High Computational Cost(Slow for Large Datasets).\n",
        "2. Sensitive to irrelevent Features & Noise.\n",
        "3. Struggles with High-Dimensional Data (Curse of Dimensionality)\n",
        "4. Imbalanced Data can Bias KNN\n",
        "5. Requires Choosing the Right K(Hyperparameter Tuning)\n",
        "6. Not Suitable for Real-Time Predictions\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "IMNECR_UGSyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. How does feature scaling affect KNN?"
      ],
      "metadata": {
        "id": "HbLzaSo9G7uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Feature scaling is essential for KNN because it is a distance-based algorithm. If the features are not scaled properly, KNN may give biased results by giving more importance to some features over others.\n",
        "\n"
      ],
      "metadata": {
        "id": "MNcr5YdhHHTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. What is PCA(Principal Component Analysis)?"
      ],
      "metadata": {
        "id": "2l70LyxTHJi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and data analysis. It helps to reduce the number of features while preserving as much information as possible.\n"
      ],
      "metadata": {
        "id": "0M3oH66UHXDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11. How does PCA Work?"
      ],
      "metadata": {
        "id": "YnUb7HySHZeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- How PCA Works:-\n",
        "\n",
        "```\n",
        "1. Standardize the Data\n",
        "2. Compute the Covariance Matrix\n",
        "3. Calculate Eigenvalues & Eigenvectors\n",
        "4. Select Principal Components\n",
        "5. Transform the Data\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FilhniPoHhFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12. What is the geometric intuition behind PCA?"
      ],
      "metadata": {
        "id": "iTwoGhdSH3o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Principal Component Analysis (PCA) is a transformation that finds the best axes to represent data in a lower-dimensional space while preserving the most variation.\n",
        "\n"
      ],
      "metadata": {
        "id": "QHiMht_jIE6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13. What is the difference b/w Feature Selection and Feature Extraction?"
      ],
      "metadata": {
        "id": "c3czDRqVIPNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Feature Selection:-\n",
        "\n",
        "```\n",
        "Defination :- Feature Selection selects a subset of the original features without transforming them.\n",
        "* It removes irrelevant, redundant, or less important features to improve model performance.\n",
        "* Keeps original features but removes the unnecessary ones.\n",
        "* The selected features remain interpretable.\n",
        "```\n",
        "\n",
        "####Feature Extraction:-\n",
        "\n",
        "```\n",
        "Defination:- Feature Extraction transforms the original features into a new set of features that capture the most important information.\n",
        "* The new features are often a combination of old ones and may not be directly interpretable.\n",
        "* Creates new features from existing ones.\n",
        "* Often used when the original features are highly correlated or too many.\n",
        "* Helps in dimensionality reduction while keeping essential information.\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5haXEIZUIehr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##14. What are Eigenvalues and Eigenvectors in PCA?"
      ],
      "metadata": {
        "id": "1St-sFS-JLJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-  Eigenvalues â€“ These are the scalars that represent how much variance (spread) is captured along each eigenvector. A larger eigenvalue means that the corresponding eigenvector (Principal Component) is more important in explaining the data's variability.\n",
        "\n",
        "#### Eigenvectors â€“ These are special vectors that indicate the directions along which the data has the most variance. In PCA, eigenvectors define the new coordinate axes (Principal Components) onto which the data is projected."
      ],
      "metadata": {
        "id": "lk5P2uXOJdPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##15. How do you decide the number of components to keep in PCA?"
      ],
      "metadata": {
        "id": "f5lwjj5DJlkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- In Principal Component Analysis (PCA), we must choose the right number of Principal Components (PCs) to keep while balancing information retention and dimensionality reduction.\n",
        "\n",
        "\n",
        "```\n",
        "1. Use the Explained Variance(Eigenvalues)\n",
        "2. Use of Scree Plot(Elbow Method).\n",
        "3. Use a Fixed Threshold(90-95% Rule)\n",
        "4. Cross-Validation(Checking Model Performance).\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zSOcdD7QJyF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##16. Can PCA be used for classification?"
      ],
      "metadata": {
        "id": "toZvj4ClKNAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- PCA itself is not a classification algorithm, but it can be used before classification to improve performance by reducing dimensionality and removing noise."
      ],
      "metadata": {
        "id": "hGq1CFEnKaQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##17. What are the limitations of PCA?"
      ],
      "metadata": {
        "id": "jyU9eTLSK8Cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Limitations of PCA:-\n",
        "\n",
        "\n",
        "```\n",
        "1. Loss of Interpretability\n",
        "2. Assumes Linearity in Data\n",
        "3. Sensitive to Scaling\n",
        "4. Sensitive to Outliers\n",
        "5. Retains Only Variance, Not class Information.\n",
        "6. Requires Large data for Reliable Results\n",
        "7. Computational Cost for Large Datasets\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "AE4dzzHWLMef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##18. How do KNN and PCA complement each other?"
      ],
      "metadata": {
        "id": "lccJwLeSKfXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- PCA helps KNN perform better by reducing dimensionality, removing noise, and improving computational efficiency."
      ],
      "metadata": {
        "id": "swhKeI7wKvxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##19. How does KNN Handle missing values in a dataset?"
      ],
      "metadata": {
        "id": "OD8PR5M7KzoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:-  K-Nearest Neighbors (KNN) can handle missing values by imputing them based on the nearest neighborsâ€™ values, using methods like mean, median, or mode imputation."
      ],
      "metadata": {
        "id": "R1ygWkbsL2fS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##20. What are the key differences b/w PCA and Linear Discriminant Analysis(LDA)?"
      ],
      "metadata": {
        "id": "KJuhJjdCL34F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ans:- Differences B/w PCA and LDA:-\n",
        "\n",
        "####PCA:-\n",
        "```\n",
        "1. Reduces dimensions by capturing maximum variance in the data\n",
        "2. Unsupervised Learning (Does not use class labels)\n",
        "3. Finds new axes (principal components) that explain the most variance in the data\n",
        "4. Dimensionality reduction, feature extraction, visualization\n",
        "```\n",
        "\n",
        "####LDA:-\n",
        "\n",
        "```\n",
        "1. Reduces dimensions while maximizing class separation\n",
        "2. Supervised Learning (Uses class labels)\n",
        "3. Finds new axes that best separate different classes\n",
        "4. Dimensionality reduction before classification\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Osz_ksA8MaOx"
      }
    }
  ]
}